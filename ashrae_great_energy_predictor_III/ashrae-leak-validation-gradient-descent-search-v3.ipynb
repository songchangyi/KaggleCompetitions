{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "Original Kernel: https://www.kaggle.com/yamsam/ashrae-leak-validation-and-more/notebook#Leak-Validation-for-public-kernels(not-used-leak-data),\n",
    "\n",
    "https://www.kaggle.com/khoongweihao/ashrae-leak-validation-bruteforce-heuristic-search\n",
    "\n",
    "Additions: Added a search method based on gradient update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All we need is Leak Validation(LV) ?\n",
    "\n",
    "* **if you like this kernel, please upvote original kernels.**\n",
    "* update site-4 and site-15\n",
    "* Turn GPU on for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this kernel is still work in progress, but i hope you can find something usefull from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16 or not. feather format does not support float16.\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 256 ms, sys: 488 ms, total: 744 ms\n",
      "Wall time: 873 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "root = Path('../input/ashrae-feather-format-for-fast-loading')\n",
    "\n",
    "#train_df = pd.read_feather(root/'train.feather')\n",
    "test_df = pd.read_feather(root/'test.feather')\n",
    "#weather_train_df = pd.read_feather(root/'weather_train.feather')\n",
    "#weather_test_df = pd.read_feather(root/'weather_test.feather')\n",
    "building_meta_df = pd.read_feather(root/'building_metadata.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i'm now using my leak data station kernel to shortcut.\n",
    "leak_df = pd.read_feather('../input/ashrae-leak-data-station/leak.feather')\n",
    "\n",
    "leak_df.fillna(0, inplace=True)\n",
    "leak_df = leak_df[(leak_df.timestamp.dt.year > 2016) & (leak_df.timestamp.dt.year < 2019)]\n",
    "leak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\n",
    "leak_df = leak_df[leak_df.building_id!=245]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leak Validation for public kernels(not used leak data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning:\n",
      "\n",
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_submission1 = pd.read_csv('../input/ashrae-kfold-lightgbm-without-leak-1-08/submission.csv', index_col=0)\n",
    "sample_submission2 = pd.read_csv('../input/ashrae-half-and-half/submission.csv', index_col=0)\n",
    "sample_submission3 = pd.read_csv('../input/ashrae-highway-kernel-route4/submission.csv', index_col=0)\n",
    "sample_submission4 = pd.read_csv('../input/ashrae-energy-prediction-using-stratified-kfold/fe2_lgbm.csv', index_col=0)\n",
    "sample_submission5 = pd.read_csv('../input/ashrae-2-lightgbm-without-leak-data/submission.csv', index_col=0)\n",
    "sample_submission6 = pd.read_csv('../input/ashrae-stratified-kfold-v4/submission_SK_drop_ws.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2505.25 MB\n",
      "Memory usage after optimization is: 1550.87 MB\n",
      "Decreased by 38.1%\n",
      "Memory usage of dataframe is 460.05 MB\n",
      "Memory usage after optimization is: 299.03 MB\n",
      "Decreased by 35.0%\n"
     ]
    }
   ],
   "source": [
    "test_df['pred1'] = sample_submission1.meter_reading\n",
    "test_df['pred2'] = sample_submission2.meter_reading\n",
    "test_df['pred3'] = sample_submission3.meter_reading\n",
    "test_df['pred4'] = sample_submission4.meter_reading\n",
    "test_df['pred5'] = sample_submission5.meter_reading\n",
    "test_df['pred6'] = sample_submission6.meter_reading\n",
    "\n",
    "test_df.loc[test_df.pred1<0, 'pred1'] = 0\n",
    "test_df.loc[test_df.pred2<0, 'pred2'] = 0\n",
    "test_df.loc[test_df.pred3<0, 'pred3'] = 0\n",
    "test_df.loc[test_df.pred4<0, 'pred4'] = 0\n",
    "test_df.loc[test_df.pred5<0, 'pred5'] = 0 \n",
    "test_df.loc[test_df.pred6<0, 'pred6'] = 0 \n",
    "\n",
    "del  sample_submission1,  sample_submission2,  sample_submission3,  sample_submission4,  sample_submission5,  sample_submission6\n",
    "gc.collect()\n",
    "\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "leak_df = reduce_mem_usage(leak_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df = leak_df.merge(test_df[['building_id', 'meter', 'timestamp', \n",
    "                                 'pred1', 'pred2', 'pred3', 'pred4', 'pred5', 'pred6',\n",
    "                                 'row_id']], left_on = ['building_id', 'meter', 'timestamp'], right_on = ['building_id', 'meter', 'timestamp'], how = \"left\")\n",
    "leak_df = leak_df.merge(building_meta_df[['building_id', 'site_id']], on='building_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df['pred1_l1p'] = np.log1p(leak_df.pred1)\n",
    "leak_df['pred2_l1p'] = np.log1p(leak_df.pred2)\n",
    "leak_df['pred3_l1p'] = np.log1p(leak_df.pred3)\n",
    "leak_df['pred4_l1p'] = np.log1p(leak_df.pred4)\n",
    "leak_df['pred5_l1p'] = np.log1p(leak_df.pred5)\n",
    "leak_df['pred6_l1p'] = np.log1p(leak_df.pred6)\n",
    "leak_df['meter_reading_l1p'] = np.log1p(leak_df.meter_reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred1_l1p</th>\n",
       "      <th>pred2_l1p</th>\n",
       "      <th>pred3_l1p</th>\n",
       "      <th>pred4_l1p</th>\n",
       "      <th>pred5_l1p</th>\n",
       "      <th>pred6_l1p</th>\n",
       "      <th>meter_reading_l1p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred1_l1p</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986133</td>\n",
       "      <td>0.988106</td>\n",
       "      <td>0.977856</td>\n",
       "      <td>0.988009</td>\n",
       "      <td>0.977550</td>\n",
       "      <td>0.862842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred2_l1p</th>\n",
       "      <td>0.986133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977993</td>\n",
       "      <td>0.967448</td>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.967188</td>\n",
       "      <td>0.856097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred3_l1p</th>\n",
       "      <td>0.988106</td>\n",
       "      <td>0.977993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966034</td>\n",
       "      <td>0.979566</td>\n",
       "      <td>0.965632</td>\n",
       "      <td>0.859570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred4_l1p</th>\n",
       "      <td>0.977856</td>\n",
       "      <td>0.967448</td>\n",
       "      <td>0.966034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969502</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.867698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred5_l1p</th>\n",
       "      <td>0.988009</td>\n",
       "      <td>0.978677</td>\n",
       "      <td>0.979566</td>\n",
       "      <td>0.969502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969328</td>\n",
       "      <td>0.861896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred6_l1p</th>\n",
       "      <td>0.977550</td>\n",
       "      <td>0.967188</td>\n",
       "      <td>0.965632</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.969328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meter_reading_l1p</th>\n",
       "      <td>0.862842</td>\n",
       "      <td>0.856097</td>\n",
       "      <td>0.859570</td>\n",
       "      <td>0.867698</td>\n",
       "      <td>0.861896</td>\n",
       "      <td>0.867730</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pred1_l1p  pred2_l1p  pred3_l1p  pred4_l1p  pred5_l1p  \\\n",
       "pred1_l1p           1.000000   0.986133   0.988106   0.977856   0.988009   \n",
       "pred2_l1p           0.986133   1.000000   0.977993   0.967448   0.978677   \n",
       "pred3_l1p           0.988106   0.977993   1.000000   0.966034   0.979566   \n",
       "pred4_l1p           0.977856   0.967448   0.966034   1.000000   0.969502   \n",
       "pred5_l1p           0.988009   0.978677   0.979566   0.969502   1.000000   \n",
       "pred6_l1p           0.977550   0.967188   0.965632   0.999776   0.969328   \n",
       "meter_reading_l1p   0.862842   0.856097   0.859570   0.867698   0.861896   \n",
       "\n",
       "                   pred6_l1p  meter_reading_l1p  \n",
       "pred1_l1p           0.977550           0.862842  \n",
       "pred2_l1p           0.967188           0.856097  \n",
       "pred3_l1p           0.965632           0.859570  \n",
       "pred4_l1p           0.999776           0.867698  \n",
       "pred5_l1p           0.969328           0.861896  \n",
       "pred6_l1p           1.000000           0.867730  \n",
       "meter_reading_l1p   0.867730           1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best : 0.867698\n",
    "leak_df[['pred1_l1p', 'pred2_l1p', 'pred3_l1p', 'pred4_l1p', 'pred5_l1p', 'pred6_l1p',\n",
    "         'meter_reading_l1p']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Search by using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ \\- input \n",
    "\n",
    "$y$ \\- target\n",
    "\n",
    "$w$ \\- weights\n",
    "\n",
    "Let $f(x)=w^\\top x$, we want to minimize\n",
    "\n",
    "$$L(x,y)=(\\log(f(x)+1)-\\log(y+1))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "X_train = np.array([leak_df['pred1'].values,\n",
    "                    leak_df['pred2'].values, \n",
    "                    leak_df['pred3'].values,\n",
    "                    #leak_df['pred4'].values,\n",
    "                    leak_df['pred6'].values\n",
    "                   ]).T\n",
    "y_train = leak_df.meter_reading_l1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 0.9793186783790588\n",
      "step: 1 0.9303372502326965\n",
      "step: 2 0.918653666973114\n",
      "step: 3 0.917375922203064\n",
      "step: 4 0.9171513319015503\n",
      "step: 5 0.9169761538505554\n",
      "step: 6 0.9168081283569336\n",
      "step: 7 0.9166461229324341\n",
      "step: 8 0.9164896607398987\n",
      "step: 9 0.9163385033607483\n",
      "step: 10 0.9161926507949829\n",
      "step: 11 0.9160512685775757\n",
      "step: 12 0.9159148335456848\n",
      "step: 13 0.9157826900482178\n",
      "step: 14 0.9156548380851746\n",
      "step: 15 0.9155312180519104\n",
      "step: 16 0.9154112339019775\n",
      "step: 17 0.9152951240539551\n",
      "step: 18 0.9151828289031982\n",
      "step: 19 0.9150737524032593\n",
      "step: 20 0.9149682521820068\n",
      "step: 21 0.9148656725883484\n",
      "step: 22 0.9147664308547974\n",
      "step: 23 0.9146701097488403\n",
      "step: 24 0.9145766496658325\n",
      "step: 25 0.9144858121871948\n",
      "step: 26 0.9143977761268616\n",
      "step: 27 0.914312481880188\n",
      "step: 28 0.9142294526100159\n",
      "step: 29 0.9141489267349243\n",
      "step: 30 0.9140706658363342\n",
      "step: 31 0.9139946103096008\n",
      "step: 32 0.9139207601547241\n",
      "step: 33 0.9138489961624146\n",
      "step: 34 0.9137791991233826\n",
      "step: 35 0.913711428642273\n",
      "step: 36 0.9136456251144409\n",
      "step: 37 0.9135815501213074\n",
      "step: 38 0.9135192632675171\n",
      "step: 39 0.9134588241577148\n",
      "step: 40 0.9133997559547424\n",
      "step: 41 0.9133425354957581\n",
      "step: 42 0.9132867455482483\n",
      "step: 43 0.9132325053215027\n",
      "step: 44 0.9131797552108765\n",
      "step: 45 0.9131285548210144\n",
      "step: 46 0.9130785465240479\n",
      "step: 47 0.9130299687385559\n",
      "step: 48 0.912982702255249\n",
      "step: 49 0.9129365682601929\n",
      "step: 50 0.9128916263580322\n",
      "step: 51 0.9128481149673462\n",
      "step: 52 0.9128055572509766\n",
      "step: 53 0.9127641320228577\n",
      "step: 54 0.9127238988876343\n",
      "step: 55 0.9126846790313721\n",
      "step: 56 0.9126463532447815\n",
      "step: 57 0.9126091599464417\n",
      "step: 58 0.9125728011131287\n",
      "step: 59 0.9125375151634216\n",
      "step: 60 0.9125030636787415\n",
      "step: 61 0.9124693870544434\n",
      "step: 62 0.912436842918396\n",
      "step: 63 0.9124050140380859\n",
      "step: 64 0.9123737812042236\n",
      "step: 65 0.9123436212539673\n",
      "step: 66 0.9123141765594482\n",
      "step: 67 0.912285327911377\n",
      "step: 68 0.9122572541236877\n",
      "step: 69 0.9122298359870911\n",
      "step: 70 0.9122032523155212\n",
      "step: 71 0.9121772646903992\n",
      "step: 72 0.9121519327163696\n",
      "step: 73 0.9121271967887878\n",
      "step: 74 0.912102997303009\n",
      "step: 75 0.9120795130729675\n",
      "step: 76 0.9120566248893738\n",
      "step: 77 0.912034273147583\n",
      "step: 78 0.9120124578475952\n",
      "step: 79 0.9119911193847656\n",
      "step: 80 0.911970317363739\n",
      "step: 81 0.9119501113891602\n",
      "step: 82 0.9119303822517395\n",
      "step: 83 0.9119109511375427\n",
      "step: 84 0.9118921160697937\n",
      "step: 85 0.9118736982345581\n",
      "step: 86 0.9118558168411255\n",
      "step: 87 0.9118382334709167\n",
      "step: 88 0.9118211269378662\n",
      "step: 89 0.9118045568466187\n",
      "step: 90 0.9117882251739502\n",
      "step: 91 0.9117722511291504\n",
      "step: 92 0.9117567539215088\n",
      "step: 93 0.9117414951324463\n",
      "step: 94 0.911726713180542\n",
      "step: 95 0.9117122292518616\n",
      "step: 96 0.9116981029510498\n",
      "step: 97 0.9116843938827515\n",
      "step: 98 0.9116708040237427\n",
      "step: 99 0.9116576313972473\n",
      "step: 100 0.911644697189331\n",
      "step: 101 0.9116321802139282\n",
      "step: 102 0.9116198420524597\n",
      "step: 103 0.9116078615188599\n",
      "step: 104 0.9115961194038391\n",
      "step: 105 0.911584734916687\n",
      "step: 106 0.9115734100341797\n",
      "step: 107 0.911562442779541\n",
      "step: 108 0.9115516543388367\n",
      "step: 109 0.911541223526001\n",
      "step: 110 0.9115309119224548\n",
      "step: 111 0.9115208983421326\n",
      "step: 112 0.9115111827850342\n",
      "step: 113 0.9115015864372253\n",
      "step: 114 0.9114922881126404\n",
      "step: 115 0.9114829897880554\n",
      "step: 116 0.9114740490913391\n",
      "step: 117 0.9114653468132019\n",
      "step: 118 0.911456823348999\n",
      "step: 119 0.9114483594894409\n",
      "step: 120 0.9114401340484619\n",
      "step: 121 0.9114320874214172\n",
      "step: 122 0.9114242196083069\n",
      "step: 123 0.9114165902137756\n",
      "step: 124 0.9114090204238892\n",
      "step: 125 0.9114016890525818\n",
      "step: 126 0.911394476890564\n",
      "step: 127 0.9113875031471252\n",
      "step: 128 0.9113805294036865\n",
      "step: 129 0.9113737344741821\n",
      "step: 130 0.9113669395446777\n",
      "step: 131 0.9113606214523315\n",
      "step: 132 0.9113541841506958\n",
      "step: 133 0.9113480448722839\n",
      "step: 134 0.9113419055938721\n",
      "step: 135 0.911335825920105\n",
      "step: 136 0.9113300442695618\n",
      "step: 137 0.9113243222236633\n",
      "step: 138 0.9113186597824097\n",
      "step: 139 0.9113131761550903\n",
      "step: 140 0.9113078117370605\n",
      "step: 141 0.911302387714386\n",
      "step: 142 0.9112973809242249\n",
      "step: 143 0.9112921953201294\n",
      "step: 144 0.9112873077392578\n",
      "step: 145 0.9112823605537415\n",
      "step: 146 0.911277711391449\n",
      "step: 147 0.9112728834152222\n",
      "step: 148 0.9112682342529297\n",
      "step: 149 0.9112638831138611\n",
      "step: 150 0.9112593531608582\n",
      "step: 151 0.9112550616264343\n",
      "step: 152 0.9112508296966553\n",
      "step: 153 0.9112467169761658\n",
      "step: 154 0.9112425446510315\n",
      "step: 155 0.9112385511398315\n",
      "step: 156 0.9112346172332764\n",
      "step: 157 0.9112306833267212\n",
      "step: 158 0.9112269878387451\n",
      "step: 159 0.9112232327461243\n",
      "step: 160 0.911219596862793\n",
      "step: 161 0.9112158417701721\n",
      "step: 162 0.9112124443054199\n",
      "step: 163 0.911208987236023\n",
      "step: 164 0.9112056493759155\n",
      "step: 165 0.9112022519111633\n",
      "step: 166 0.9111990332603455\n",
      "step: 167 0.9111957550048828\n",
      "step: 168 0.9111927151679993\n",
      "step: 169 0.9111895561218262\n",
      "step: 170 0.9111863970756531\n",
      "step: 171 0.9111834168434143\n",
      "step: 172 0.9111806154251099\n",
      "step: 173 0.9111778140068054\n",
      "step: 174 0.9111749529838562\n",
      "step: 175 0.9111721515655518\n",
      "step: 176 0.9111693501472473\n",
      "step: 177 0.9111666679382324\n",
      "step: 178 0.9111640453338623\n",
      "step: 179 0.911161482334137\n",
      "step: 180 0.9111588597297668\n",
      "step: 181 0.9111562967300415\n",
      "step: 182 0.9111538529396057\n",
      "step: 183 0.9111515879631042\n",
      "step: 184 0.9111492037773132\n",
      "step: 185 0.9111467003822327\n",
      "step: 186 0.9111443758010864\n",
      "step: 187 0.911142110824585\n",
      "step: 188 0.911139965057373\n",
      "step: 189 0.9111378788948059\n",
      "step: 190 0.9111356735229492\n",
      "step: 191 0.9111335277557373\n",
      "step: 192 0.9111313819885254\n",
      "step: 193 0.911129355430603\n",
      "step: 194 0.9111274480819702\n",
      "step: 195 0.9111253619194031\n",
      "step: 196 0.9111234545707703\n",
      "step: 197 0.9111214876174927\n",
      "step: 198 0.9111197590827942\n",
      "step: 199 0.9111177325248718\n",
      "step: 200 0.9111159443855286\n",
      "step: 201 0.9111142158508301\n",
      "step: 202 0.911112368106842\n",
      "step: 203 0.911110520362854\n",
      "step: 204 0.9111089706420898\n",
      "step: 205 0.9111071228981018\n",
      "step: 206 0.9111055731773376\n",
      "step: 207 0.9111039638519287\n",
      "step: 208 0.9111022353172302\n",
      "step: 209 0.9111006855964661\n",
      "step: 210 0.9110991954803467\n",
      "step: 211 0.9110977053642273\n",
      "step: 212 0.9110960364341736\n",
      "step: 213 0.911094605922699\n",
      "step: 214 0.9110931158065796\n",
      "step: 215 0.9110917448997498\n",
      "step: 216 0.9110903739929199\n",
      "step: 217 0.9110888242721558\n",
      "step: 218 0.9110875129699707\n",
      "step: 219 0.9110861420631409\n",
      "step: 220 0.9110848307609558\n",
      "step: 221 0.9110835194587708\n",
      "step: 222 0.9110823273658752\n",
      "step: 223 0.9110808372497559\n",
      "step: 224 0.9110795855522156\n",
      "step: 225 0.9110783934593201\n",
      "step: 226 0.9110773205757141\n",
      "step: 227 0.9110759496688843\n",
      "step: 228 0.911074697971344\n",
      "step: 229 0.9110735654830933\n",
      "step: 230 0.9110723733901978\n",
      "step: 231 0.911071240901947\n",
      "step: 232 0.9110701680183411\n",
      "step: 233 0.9110690951347351\n",
      "step: 234 0.9110680222511292\n",
      "step: 235 0.9110669493675232\n",
      "step: 236 0.9110658764839172\n",
      "step: 237 0.911064863204956\n",
      "step: 238 0.9110636711120605\n",
      "step: 239 0.9110627174377441\n",
      "step: 240 0.9110617637634277\n",
      "step: 241 0.911060631275177\n",
      "step: 242 0.9110597372055054\n",
      "step: 243 0.911058783531189\n",
      "step: 244 0.9110578894615173\n",
      "step: 245 0.9110568761825562\n",
      "step: 246 0.9110559821128845\n",
      "step: 247 0.9110550880432129\n",
      "step: 248 0.9110541343688965\n",
      "step: 249 0.9110531806945801\n",
      "step: 250 0.911052405834198\n",
      "step: 251 0.9110515713691711\n",
      "step: 252 0.9110506772994995\n",
      "step: 253 0.9110499024391174\n",
      "step: 254 0.9110491275787354\n",
      "step: 255 0.911048173904419\n",
      "step: 256 0.9110475182533264\n",
      "step: 257 0.9110466837882996\n",
      "step: 258 0.9110458493232727\n",
      "step: 259 0.9110451340675354\n",
      "step: 260 0.9110443592071533\n",
      "step: 261 0.9110435247421265\n",
      "step: 262 0.9110428690910339\n",
      "step: 263 0.9110421538352966\n",
      "step: 264 0.9110413789749146\n",
      "step: 265 0.911040723323822\n",
      "step: 266 0.9110399484634399\n",
      "step: 267 0.9110392928123474\n",
      "step: 268 0.9110386371612549\n",
      "step: 269 0.9110379815101624\n",
      "step: 270 0.9110373258590698\n",
      "step: 271 0.9110365509986877\n",
      "step: 272 0.9110360145568848\n",
      "step: 273 0.9110353589057922\n",
      "step: 274 0.9110347628593445\n",
      "step: 275 0.9110339879989624\n",
      "step: 276 0.9110334515571594\n",
      "step: 277 0.9110327363014221\n",
      "step: 278 0.9110321998596191\n",
      "step: 279 0.9110316634178162\n",
      "step: 280 0.9110310077667236\n",
      "step: 281 0.9110305905342102\n",
      "step: 282 0.9110299348831177\n",
      "step: 283 0.9110292792320251\n",
      "step: 284 0.9110288619995117\n",
      "step: 285 0.9110281467437744\n",
      "step: 286 0.911027729511261\n",
      "step: 287 0.9110271334648132\n",
      "step: 288 0.911026656627655\n",
      "step: 289 0.9110260605812073\n",
      "step: 290 0.9110256433486938\n",
      "step: 291 0.9110251069068909\n",
      "step: 292 0.9110246300697327\n",
      "step: 293 0.9110241532325745\n",
      "step: 294 0.9110236167907715\n",
      "step: 295 0.9110230803489685\n",
      "step: 296 0.9110226631164551\n",
      "step: 297 0.9110221266746521\n",
      "step: 298 0.9110216498374939\n",
      "step: 299 0.9110212326049805\n",
      "step: 300 0.911020815372467\n",
      "step: 301 0.9110203385353088\n",
      "step: 302 0.9110199809074402\n",
      "step: 303 0.911019504070282\n",
      "step: 304 0.9110190272331238\n",
      "step: 305 0.9110186100006104\n",
      "step: 306 0.9110181927680969\n",
      "step: 307 0.9110177755355835\n",
      "step: 308 0.9110173583030701\n",
      "step: 309 0.9110169410705566\n",
      "step: 310 0.911016583442688\n",
      "step: 311 0.9110161662101746\n",
      "step: 312 0.9110157489776611\n",
      "step: 313 0.9110153317451477\n",
      "step: 314 0.9110150337219238\n",
      "step: 315 0.9110146760940552\n",
      "step: 316 0.9110142588615417\n",
      "step: 317 0.9110140204429626\n",
      "step: 318 0.9110136032104492\n",
      "step: 319 0.9110131859779358\n",
      "step: 320 0.9110127687454224\n",
      "step: 321 0.9110123515129089\n",
      "step: 322 0.9110121130943298\n",
      "step: 323 0.911011815071106\n",
      "step: 324 0.9110114574432373\n",
      "step: 325 0.9110112190246582\n",
      "step: 326 0.9110108017921448\n",
      "step: 327 0.9110104441642761\n",
      "step: 328 0.911010205745697\n",
      "step: 329 0.9110099077224731\n",
      "step: 330 0.9110095500946045\n",
      "step: 331 0.9110091924667358\n",
      "step: 332 0.911008894443512\n",
      "step: 333 0.9110086560249329\n",
      "step: 334 0.9110082387924194\n",
      "step: 335 0.9110080599784851\n",
      "step: 336 0.911007821559906\n",
      "step: 337 0.9110074639320374\n",
      "step: 338 0.9110071659088135\n",
      "step: 339 0.9110068678855896\n",
      "step: 340 0.9110065698623657\n",
      "step: 341 0.9110063314437866\n",
      "step: 342 0.9110060930252075\n",
      "step: 343 0.9110057353973389\n",
      "step: 344 0.9110055565834045\n",
      "step: 345 0.9110052585601807\n",
      "step: 346 0.9110049605369568\n",
      "step: 347 0.9110048413276672\n",
      "step: 348 0.9110044836997986\n",
      "step: 349 0.9110043048858643\n",
      "step: 350 0.9110040068626404\n",
      "step: 351 0.9110037684440613\n",
      "step: 352 0.911003589630127\n",
      "step: 353 0.9110033512115479\n",
      "step: 354 0.911003053188324\n",
      "step: 355 0.9110028147697449\n",
      "step: 356 0.9110025763511658\n",
      "step: 357 0.9110023379325867\n",
      "step: 358 0.9110021591186523\n",
      "step: 359 0.911001980304718\n",
      "step: 360 0.9110017418861389\n",
      "step: 361 0.9110015630722046\n",
      "step: 362 0.9110012650489807\n",
      "step: 363 0.9110010862350464\n",
      "step: 364 0.9110008478164673\n",
      "step: 365 0.9110007286071777\n",
      "step: 366 0.9110004305839539\n",
      "step: 367 0.9110002517700195\n",
      "step: 368 0.9110000133514404\n",
      "step: 369 0.9109999537467957\n",
      "step: 370 0.9109996557235718\n",
      "step: 371 0.910999596118927\n",
      "step: 372 0.9109992384910583\n",
      "step: 373 0.9109991788864136\n",
      "step: 374 0.9109988212585449\n",
      "step: 375 0.9109987616539001\n",
      "step: 376 0.9109987020492554\n",
      "step: 377 0.9109984636306763\n",
      "step: 378 0.9109982848167419\n",
      "step: 379 0.9109980463981628\n",
      "step: 380 0.9109979271888733\n",
      "step: 381 0.910997748374939\n",
      "step: 382 0.9109976291656494\n",
      "step: 383 0.9109974503517151\n",
      "step: 384 0.9109972715377808\n",
      "step: 385 0.9109970927238464\n",
      "step: 386 0.9109969139099121\n",
      "step: 387 0.9109967947006226\n",
      "step: 388 0.910996675491333\n",
      "step: 389 0.9109964370727539\n",
      "step: 390 0.9109962582588196\n",
      "step: 391 0.9109961986541748\n",
      "step: 392 0.9109960198402405\n",
      "step: 393 0.9109958410263062\n",
      "step: 394 0.910995602607727\n",
      "step: 395 0.910995602607727\n",
      "step: 396 0.9109954237937927\n",
      "step: 397 0.9109953045845032\n",
      "step: 398 0.9109951853752136\n",
      "step: 399 0.9109950065612793\n",
      "step: 400 0.9109947681427002\n",
      "step: 401 0.9109947681427002\n",
      "step: 402 0.9109946489334106\n",
      "step: 403 0.9109945297241211\n",
      "step: 404 0.9109944701194763\n",
      "step: 405 0.9109942317008972\n",
      "step: 406 0.9109940528869629\n",
      "step: 407 0.9109940528869629\n",
      "step: 408 0.9109938740730286\n",
      "step: 409 0.9109938144683838\n",
      "step: 410 0.9109936356544495\n",
      "step: 411 0.9109934568405151\n",
      "step: 412 0.9109935164451599\n",
      "step: 413 0.9109933972358704\n",
      "step: 414 0.9109932780265808\n",
      "step: 415 0.9109930992126465\n",
      "step: 416 0.9109930992126465\n",
      "step: 417 0.9109929800033569\n",
      "step: 418 0.9109929800033569\n",
      "step: 419 0.9109929800033569\n",
      "step: 420 0.9109928607940674\n",
      "step: 421 0.9109928607940674\n",
      "step: 422 0.9109928607940674\n",
      "step: 423 0.9109928011894226\n",
      "step: 424 0.9109927415847778\n",
      "step: 425 0.9109926223754883\n",
      "step: 426 0.9109925627708435\n",
      "step: 427 0.910992443561554\n",
      "step: 428 0.910992443561554\n",
      "step: 429 0.9109923839569092\n",
      "step: 430 0.9109923243522644\n",
      "step: 431 0.9109923243522644\n",
      "step: 432 0.9109923243522644\n",
      "step: 433 0.9109922051429749\n",
      "step: 434 0.9109921455383301\n",
      "step: 435 0.9109921455383301\n",
      "step: 436 0.9109919667243958\n",
      "step: 437 0.9109919667243958\n",
      "step: 438 0.910991907119751\n",
      "step: 439 0.910991907119751\n",
      "step: 440 0.9109917879104614\n",
      "step: 441 0.9109917283058167\n",
      "step: 442 0.9109917879104614\n",
      "step: 443 0.9109917879104614\n",
      "step: 444 0.9109917879104614\n",
      "step: 445 0.9109917283058167\n",
      "step: 446 0.9109916090965271\n",
      "step: 447 0.9109916090965271\n",
      "step: 448 0.9109916090965271\n",
      "step: 449 0.9109916090965271\n",
      "step: 450 0.9109915494918823\n",
      "step: 451 0.9109915494918823\n",
      "step: 452 0.9109914898872375\n",
      "step: 453 0.9109914898872375\n",
      "step: 454 0.9109914898872375\n",
      "step: 455 0.9109914898872375\n",
      "step: 456 0.910991370677948\n",
      "step: 457 0.910991370677948\n",
      "step: 458 0.9109913110733032\n",
      "step: 459 0.910991370677948\n",
      "step: 460 0.9109913110733032\n",
      "step: 461 0.9109913110733032\n",
      "step: 462 0.9109913110733032\n",
      "step: 463 0.9109913110733032\n",
      "step: 464 0.9109913110733032\n",
      "step: 465 0.9109913110733032\n",
      "step: 466 0.9109911918640137\n",
      "step: 467 0.9109913110733032\n",
      "step: 468 0.9109911918640137\n",
      "step: 469 0.9109911918640137\n",
      "step: 470 0.9109913110733032\n",
      "step: 471 0.9109911918640137\n",
      "step: 472 0.9109911918640137\n",
      "step: 473 0.9109911918640137\n",
      "step: 474 0.9109911918640137\n",
      "step: 475 0.9109911918640137\n",
      "step: 476 0.9109911918640137\n",
      "step: 477 0.9109911918640137\n",
      "step: 478 0.9109911918640137\n",
      "step: 479 0.9109913110733032\n",
      "step: 480 0.9109913110733032\n",
      "step: 481 0.9109911918640137\n",
      "step: 482 0.9109911918640137\n",
      "step: 483 0.9109911918640137\n",
      "step: 484 0.9109911918640137\n",
      "step: 485 0.9109911918640137\n",
      "step: 486 0.9109911918640137\n",
      "step: 487 0.9109911918640137\n",
      "step: 488 0.9109911322593689\n",
      "step: 489 0.9109911918640137\n",
      "step: 490 0.9109911918640137\n",
      "step: 491 0.9109911918640137\n",
      "step: 492 0.9109913110733032\n",
      "step: 493 0.9109913110733032\n",
      "step: 494 0.9109913110733032\n",
      "step: 495 0.9109913110733032\n",
      "step: 496 0.9109913110733032\n",
      "step: 497 0.9109911918640137\n",
      "step: 498 0.9109911918640137\n",
      "step: 499 0.9109911918640137\n",
      "step: 500 0.9109911918640137\n",
      "step: 501 0.9109911918640137\n",
      "step: 502 0.9109911918640137\n",
      "step: 503 0.9109911918640137\n",
      "step: 504 0.9109911918640137\n",
      "step: 505 0.9109911918640137\n",
      "step: 506 0.9109911918640137\n",
      "step: 507 0.9109911918640137\n",
      "step: 508 0.9109911918640137\n",
      "step: 509 0.9109911918640137\n",
      "step: 510 0.9109911918640137\n",
      "step: 511 0.9109911918640137\n",
      "step: 512 0.9109911918640137\n",
      "step: 513 0.9109911918640137\n",
      "step: 514 0.9109911918640137\n",
      "step: 515 0.9109911918640137\n",
      "step: 516 0.9109911918640137\n",
      "step: 517 0.9109911918640137\n",
      "step: 518 0.9109911918640137\n",
      "step: 519 0.9109911918640137\n",
      "step: 520 0.9109911918640137\n",
      "step: 521 0.9109911918640137\n",
      "step: 522 0.9109911918640137\n",
      "step: 523 0.9109911918640137\n",
      "step: 524 0.9109911918640137\n",
      "step: 525 0.9109911918640137\n",
      "step: 526 0.9109911918640137\n",
      "step: 527 0.9109911918640137\n",
      "step: 528 0.9109911918640137\n",
      "step: 529 0.9109911918640137\n",
      "step: 530 0.9109911918640137\n",
      "step: 531 0.9109911918640137\n",
      "step: 532 0.9109911918640137\n",
      "step: 533 0.9109911322593689\n",
      "step: 534 0.9109911322593689\n",
      "step: 535 0.9109911322593689\n",
      "step: 536 0.9109911322593689\n",
      "step: 537 0.9109911322593689\n",
      "step: 538 0.9109911322593689\n",
      "step: 539 0.9109911322593689\n",
      "step: 540 0.9109911322593689\n",
      "step: 541 0.9109911322593689\n",
      "step: 542 0.9109911322593689\n",
      "step: 543 0.9109911322593689\n",
      "step: 544 0.9109911918640137\n",
      "step: 545 0.9109911918640137\n",
      "step: 546 0.9109911918640137\n",
      "step: 547 0.9109910726547241\n",
      "step: 548 0.9109911918640137\n",
      "step: 549 0.9109911918640137\n",
      "step: 550 0.9109911918640137\n",
      "step: 551 0.9109911918640137\n",
      "step: 552 0.9109911918640137\n",
      "step: 553 0.9109911918640137\n",
      "step: 554 0.9109911918640137\n",
      "step: 555 0.9109911918640137\n",
      "step: 556 0.9109911918640137\n",
      "step: 557 0.9109910726547241\n",
      "step: 558 0.9109910726547241\n",
      "step: 559 0.9109910726547241\n",
      "step: 560 0.9109910726547241\n",
      "step: 561 0.9109911918640137\n",
      "step: 562 0.9109911918640137\n",
      "step: 563 0.9109911918640137\n",
      "step: 564 0.9109911918640137\n",
      "step: 565 0.9109911918640137\n",
      "step: 566 0.9109911918640137\n",
      "step: 567 0.9109911918640137\n",
      "step: 568 0.9109911918640137\n",
      "step: 569 0.9109910726547241\n",
      "step: 570 0.9109910726547241\n",
      "step: 571 0.9109910726547241\n",
      "step: 572 0.9109910726547241\n",
      "step: 573 0.9109910726547241\n",
      "step: 574 0.9109910726547241\n",
      "step: 575 0.9109910726547241\n",
      "step: 576 0.9109910726547241\n",
      "step: 577 0.9109910726547241\n",
      "step: 578 0.9109910726547241\n",
      "step: 579 0.9109910726547241\n",
      "step: 580 0.9109910726547241\n",
      "step: 581 0.9109910726547241\n",
      "step: 582 0.9109910726547241\n",
      "step: 583 0.9109910726547241\n",
      "step: 584 0.9109910726547241\n",
      "step: 585 0.9109910726547241\n",
      "step: 586 0.9109910726547241\n",
      "step: 587 0.9109910726547241\n",
      "step: 588 0.9109910726547241\n",
      "step: 589 0.9109910726547241\n",
      "step: 590 0.9109910726547241\n",
      "step: 591 0.9109910726547241\n",
      "step: 592 0.9109910726547241\n",
      "step: 593 0.9109910726547241\n",
      "step: 594 0.9109910726547241\n",
      "step: 595 0.9109910726547241\n",
      "step: 596 0.9109910726547241\n",
      "step: 597 0.9109910726547241\n",
      "step: 598 0.9109910726547241\n",
      "step: 599 0.9109910726547241\n",
      "step: 600 0.9109911918640137\n",
      "step: 601 0.9109910726547241\n",
      "step: 602 0.9109910726547241\n",
      "step: 603 0.9109911918640137\n",
      "step: 604 0.9109910726547241\n",
      "step: 605 0.9109910726547241\n",
      "step: 606 0.9109910726547241\n",
      "step: 607 0.9109910726547241\n",
      "step: 608 0.9109910726547241\n",
      "step: 609 0.9109910726547241\n",
      "step: 610 0.9109910726547241\n",
      "step: 611 0.9109910726547241\n",
      "step: 612 0.9109910726547241\n",
      "step: 613 0.9109910726547241\n",
      "step: 614 0.9109910726547241\n",
      "step: 615 0.9109910726547241\n",
      "step: 616 0.9109910726547241\n",
      "step: 617 0.9109910726547241\n",
      "step: 618 0.9109910726547241\n",
      "step: 619 0.9109910726547241\n",
      "step: 620 0.9109911918640137\n",
      "step: 621 0.9109911918640137\n",
      "step: 622 0.9109910726547241\n",
      "step: 623 0.9109910726547241\n",
      "step: 624 0.9109910726547241\n",
      "step: 625 0.9109910726547241\n",
      "step: 626 0.9109910726547241\n",
      "step: 627 0.9109910726547241\n",
      "step: 628 0.9109910726547241\n",
      "step: 629 0.9109910726547241\n",
      "step: 630 0.9109910726547241\n",
      "step: 631 0.9109911918640137\n",
      "step: 632 0.9109911918640137\n",
      "step: 633 0.9109911918640137\n",
      "step: 634 0.9109911918640137\n",
      "step: 635 0.9109911918640137\n",
      "step: 636 0.9109911918640137\n",
      "step: 637 0.9109911918640137\n",
      "step: 638 0.9109911918640137\n",
      "step: 639 0.9109911918640137\n",
      "step: 640 0.9109911918640137\n",
      "step: 641 0.9109911918640137\n",
      "step: 642 0.9109911918640137\n",
      "step: 643 0.9109911918640137\n",
      "step: 644 0.9109911918640137\n",
      "step: 645 0.9109911918640137\n",
      "step: 646 0.9109911918640137\n",
      "step: 647 0.9109911918640137\n",
      "step: 648 0.9109911918640137\n",
      "step: 649 0.9109911918640137\n",
      "step: 650 0.9109911918640137\n",
      "step: 651 0.9109911918640137\n",
      "step: 652 0.9109911918640137\n",
      "step: 653 0.9109911918640137\n",
      "step: 654 0.9109911918640137\n",
      "step: 655 0.9109911918640137\n",
      "step: 656 0.9109911918640137\n",
      "step: 657 0.9109911918640137\n",
      "step: 658 0.9109911918640137\n",
      "step: 659 0.9109911918640137\n",
      "step: 660 0.9109911918640137\n",
      "step: 661 0.9109911918640137\n",
      "step: 662 0.9109911918640137\n",
      "step: 663 0.9109911918640137\n",
      "step: 664 0.9109911918640137\n",
      "step: 665 0.9109911918640137\n",
      "step: 666 0.9109911918640137\n",
      "step: 667 0.9109911918640137\n",
      "step: 668 0.9109911918640137\n",
      "step: 669 0.9109911918640137\n",
      "step: 670 0.9109911918640137\n",
      "step: 671 0.9109911918640137\n",
      "step: 672 0.9109911918640137\n",
      "step: 673 0.9109911918640137\n",
      "step: 674 0.9109911918640137\n",
      "step: 675 0.9109911918640137\n",
      "step: 676 0.9109911918640137\n",
      "step: 677 0.9109911918640137\n",
      "step: 678 0.9109911918640137\n",
      "step: 679 0.9109911918640137\n",
      "step: 680 0.9109911918640137\n",
      "step: 681 0.9109911918640137\n",
      "step: 682 0.9109911918640137\n",
      "step: 683 0.9109911918640137\n",
      "step: 684 0.9109911918640137\n",
      "step: 685 0.9109911918640137\n",
      "step: 686 0.9109911918640137\n",
      "step: 687 0.9109911918640137\n",
      "step: 688 0.9109911918640137\n",
      "step: 689 0.9109911918640137\n",
      "step: 690 0.9109911918640137\n",
      "step: 691 0.9109911918640137\n",
      "step: 692 0.9109911918640137\n",
      "step: 693 0.9109911918640137\n",
      "step: 694 0.9109911918640137\n",
      "step: 695 0.9109911918640137\n",
      "step: 696 0.9109911918640137\n",
      "step: 697 0.9109911918640137\n",
      "step: 698 0.9109911918640137\n",
      "step: 699 0.9109911918640137\n",
      "step: 700 0.9109911918640137\n",
      "step: 701 0.9109911918640137\n",
      "step: 702 0.9109911918640137\n",
      "step: 703 0.9109911918640137\n",
      "step: 704 0.9109911918640137\n",
      "step: 705 0.9109911918640137\n",
      "step: 706 0.9109911918640137\n",
      "step: 707 0.9109911918640137\n",
      "step: 708 0.9109911918640137\n",
      "step: 709 0.9109911918640137\n",
      "step: 710 0.9109911918640137\n",
      "step: 711 0.9109911918640137\n",
      "step: 712 0.9109911918640137\n",
      "step: 713 0.9109911918640137\n",
      "step: 714 0.9109911918640137\n",
      "step: 715 0.9109911918640137\n",
      "step: 716 0.9109911918640137\n",
      "step: 717 0.9109911918640137\n",
      "step: 718 0.9109911918640137\n",
      "step: 719 0.9109911918640137\n",
      "step: 720 0.9109911918640137\n",
      "step: 721 0.9109911918640137\n",
      "step: 722 0.9109911918640137\n",
      "step: 723 0.9109911918640137\n",
      "step: 724 0.9109911918640137\n",
      "step: 725 0.9109911918640137\n",
      "step: 726 0.9109911918640137\n",
      "step: 727 0.9109911918640137\n",
      "step: 728 0.9109911918640137\n",
      "step: 729 0.9109911918640137\n",
      "step: 730 0.9109911918640137\n",
      "step: 731 0.9109911918640137\n",
      "step: 732 0.9109911918640137\n",
      "step: 733 0.9109911918640137\n",
      "step: 734 0.9109911918640137\n",
      "step: 735 0.9109911918640137\n",
      "step: 736 0.9109911918640137\n",
      "step: 737 0.9109911918640137\n",
      "step: 738 0.9109911918640137\n",
      "step: 739 0.9109911918640137\n",
      "step: 740 0.9109911918640137\n",
      "step: 741 0.9109911918640137\n",
      "step: 742 0.9109911918640137\n",
      "step: 743 0.9109911918640137\n",
      "step: 744 0.9109911918640137\n",
      "step: 745 0.9109911918640137\n",
      "step: 746 0.9109911918640137\n",
      "step: 747 0.9109911918640137\n",
      "step: 748 0.9109911918640137\n",
      "step: 749 0.9109911918640137\n",
      "step: 750 0.9109911918640137\n",
      "step: 751 0.9109911918640137\n",
      "step: 752 0.9109911918640137\n",
      "step: 753 0.9109911918640137\n",
      "step: 754 0.9109911918640137\n",
      "step: 755 0.9109911918640137\n",
      "step: 756 0.9109911918640137\n",
      "step: 757 0.9109911918640137\n",
      "step: 758 0.9109911918640137\n",
      "step: 759 0.9109911918640137\n",
      "step: 760 0.9109911918640137\n",
      "step: 761 0.9109911918640137\n",
      "step: 762 0.9109911918640137\n",
      "step: 763 0.9109911918640137\n",
      "step: 764 0.9109911918640137\n",
      "step: 765 0.9109911918640137\n",
      "step: 766 0.9109911918640137\n",
      "step: 767 0.9109911918640137\n",
      "step: 768 0.9109911918640137\n",
      "step: 769 0.9109911918640137\n",
      "step: 770 0.9109911918640137\n",
      "step: 771 0.9109911918640137\n",
      "step: 772 0.9109911918640137\n",
      "step: 773 0.9109911918640137\n",
      "step: 774 0.9109911918640137\n",
      "step: 775 0.9109911918640137\n",
      "step: 776 0.9109911918640137\n",
      "step: 777 0.9109911918640137\n",
      "step: 778 0.9109911918640137\n",
      "step: 779 0.9109911918640137\n",
      "step: 780 0.9109911918640137\n",
      "step: 781 0.9109911918640137\n",
      "step: 782 0.9109911918640137\n",
      "step: 783 0.9109911918640137\n",
      "step: 784 0.9109911918640137\n",
      "step: 785 0.9109911918640137\n",
      "step: 786 0.9109911918640137\n",
      "step: 787 0.9109911918640137\n",
      "step: 788 0.9109911918640137\n",
      "step: 789 0.9109911918640137\n",
      "step: 790 0.9109911918640137\n",
      "step: 791 0.9109911918640137\n",
      "step: 792 0.9109911918640137\n",
      "step: 793 0.9109911918640137\n",
      "step: 794 0.9109911918640137\n",
      "step: 795 0.9109911918640137\n",
      "step: 796 0.9109911918640137\n",
      "step: 797 0.9109911918640137\n",
      "step: 798 0.9109911918640137\n",
      "step: 799 0.9109911918640137\n",
      "step: 800 0.9109911918640137\n",
      "step: 801 0.9109911918640137\n",
      "step: 802 0.9109911918640137\n",
      "step: 803 0.9109911918640137\n",
      "step: 804 0.9109911918640137\n",
      "step: 805 0.9109911918640137\n",
      "step: 806 0.9109911918640137\n",
      "step: 807 0.9109911918640137\n",
      "step: 808 0.9109911918640137\n",
      "step: 809 0.9109911918640137\n",
      "step: 810 0.9109911918640137\n",
      "step: 811 0.9109911918640137\n",
      "step: 812 0.9109911918640137\n",
      "step: 813 0.9109911918640137\n",
      "step: 814 0.9109911918640137\n",
      "step: 815 0.9109911918640137\n",
      "step: 816 0.9109911918640137\n",
      "step: 817 0.9109911918640137\n",
      "step: 818 0.9109911918640137\n",
      "step: 819 0.9109911918640137\n",
      "step: 820 0.9109911918640137\n",
      "step: 821 0.9109911918640137\n",
      "step: 822 0.9109911918640137\n",
      "step: 823 0.9109911918640137\n",
      "step: 824 0.9109911918640137\n",
      "step: 825 0.9109911918640137\n",
      "step: 826 0.9109911918640137\n",
      "step: 827 0.9109911918640137\n",
      "step: 828 0.9109911918640137\n",
      "step: 829 0.9109911918640137\n",
      "step: 830 0.9109911918640137\n",
      "step: 831 0.9109911918640137\n",
      "step: 832 0.9109911918640137\n",
      "step: 833 0.9109911918640137\n",
      "step: 834 0.9109911918640137\n",
      "step: 835 0.9109911918640137\n",
      "step: 836 0.9109911918640137\n",
      "step: 837 0.9109911918640137\n",
      "step: 838 0.9109911918640137\n",
      "step: 839 0.9109911918640137\n",
      "step: 840 0.9109911918640137\n",
      "step: 841 0.9109911918640137\n",
      "step: 842 0.9109911918640137\n",
      "step: 843 0.9109911918640137\n",
      "step: 844 0.9109911918640137\n",
      "step: 845 0.9109911918640137\n",
      "step: 846 0.9109911918640137\n",
      "step: 847 0.9109911918640137\n",
      "step: 848 0.9109911918640137\n",
      "step: 849 0.9109911918640137\n",
      "step: 850 0.9109911918640137\n",
      "step: 851 0.9109911918640137\n",
      "step: 852 0.9109911918640137\n",
      "step: 853 0.9109911918640137\n",
      "step: 854 0.9109911918640137\n",
      "step: 855 0.9109911918640137\n",
      "step: 856 0.9109911918640137\n",
      "step: 857 0.9109911918640137\n",
      "step: 858 0.9109911918640137\n",
      "step: 859 0.9109911918640137\n",
      "step: 860 0.9109911918640137\n",
      "step: 861 0.9109911918640137\n",
      "step: 862 0.9109911918640137\n",
      "step: 863 0.9109911918640137\n",
      "step: 864 0.9109911918640137\n",
      "step: 865 0.9109911918640137\n",
      "step: 866 0.9109911918640137\n",
      "step: 867 0.9109911918640137\n",
      "step: 868 0.9109911918640137\n",
      "step: 869 0.9109911918640137\n",
      "step: 870 0.9109911918640137\n",
      "step: 871 0.9109911918640137\n",
      "step: 872 0.9109911918640137\n",
      "step: 873 0.9109911918640137\n",
      "step: 874 0.9109911918640137\n",
      "step: 875 0.9109911918640137\n",
      "step: 876 0.9109911918640137\n",
      "step: 877 0.9109911918640137\n",
      "step: 878 0.9109911918640137\n",
      "step: 879 0.9109911918640137\n",
      "step: 880 0.9109911918640137\n",
      "step: 881 0.9109911918640137\n",
      "step: 882 0.9109911918640137\n",
      "step: 883 0.9109911918640137\n",
      "step: 884 0.9109911918640137\n",
      "step: 885 0.9109911918640137\n",
      "step: 886 0.9109911918640137\n",
      "step: 887 0.9109911918640137\n",
      "step: 888 0.9109911918640137\n",
      "step: 889 0.9109911918640137\n",
      "step: 890 0.9109911918640137\n",
      "step: 891 0.9109911918640137\n",
      "step: 892 0.9109911918640137\n",
      "step: 893 0.9109911918640137\n",
      "step: 894 0.9109911918640137\n",
      "step: 895 0.9109911918640137\n",
      "step: 896 0.9109911918640137\n",
      "step: 897 0.9109911918640137\n",
      "step: 898 0.9109911918640137\n",
      "step: 899 0.9109911918640137\n",
      "step: 900 0.9109911918640137\n",
      "step: 901 0.9109911918640137\n",
      "step: 902 0.9109911918640137\n",
      "step: 903 0.9109911918640137\n",
      "step: 904 0.9109911918640137\n",
      "step: 905 0.9109911918640137\n",
      "step: 906 0.9109911918640137\n",
      "step: 907 0.9109911918640137\n",
      "step: 908 0.9109911918640137\n",
      "step: 909 0.9109911918640137\n",
      "step: 910 0.9109911918640137\n",
      "step: 911 0.9109911918640137\n",
      "step: 912 0.9109911918640137\n",
      "step: 913 0.9109911918640137\n",
      "step: 914 0.9109911918640137\n",
      "step: 915 0.9109911918640137\n",
      "step: 916 0.9109911918640137\n",
      "step: 917 0.9109911918640137\n",
      "step: 918 0.9109911918640137\n",
      "step: 919 0.9109911918640137\n",
      "step: 920 0.9109911918640137\n",
      "step: 921 0.9109911918640137\n",
      "step: 922 0.9109911918640137\n",
      "step: 923 0.9109911918640137\n",
      "step: 924 0.9109911918640137\n",
      "step: 925 0.9109911918640137\n",
      "step: 926 0.9109911918640137\n",
      "step: 927 0.9109911918640137\n",
      "step: 928 0.9109911918640137\n",
      "step: 929 0.9109911918640137\n",
      "step: 930 0.9109911918640137\n",
      "step: 931 0.9109911918640137\n",
      "step: 932 0.9109911918640137\n",
      "step: 933 0.9109911918640137\n",
      "step: 934 0.9109911918640137\n",
      "step: 935 0.9109911918640137\n",
      "step: 936 0.9109911918640137\n",
      "step: 937 0.9109911918640137\n",
      "step: 938 0.9109911918640137\n",
      "step: 939 0.9109911918640137\n",
      "step: 940 0.9109911918640137\n",
      "step: 941 0.9109911918640137\n",
      "step: 942 0.9109911918640137\n",
      "step: 943 0.9109911918640137\n",
      "step: 944 0.9109911918640137\n",
      "step: 945 0.9109911918640137\n",
      "step: 946 0.9109911918640137\n",
      "step: 947 0.9109911918640137\n",
      "step: 948 0.9109911918640137\n",
      "step: 949 0.9109911918640137\n",
      "step: 950 0.9109911918640137\n",
      "step: 951 0.9109911918640137\n",
      "step: 952 0.9109911918640137\n",
      "step: 953 0.9109911918640137\n",
      "step: 954 0.9109911918640137\n",
      "step: 955 0.9109911918640137\n",
      "step: 956 0.9109911918640137\n",
      "step: 957 0.9109911918640137\n",
      "step: 958 0.9109911918640137\n",
      "step: 959 0.9109911918640137\n",
      "step: 960 0.9109911918640137\n",
      "step: 961 0.9109911918640137\n",
      "step: 962 0.9109911918640137\n",
      "step: 963 0.9109911918640137\n",
      "step: 964 0.9109911918640137\n",
      "step: 965 0.9109911918640137\n",
      "step: 966 0.9109911918640137\n",
      "step: 967 0.9109911918640137\n",
      "step: 968 0.9109911918640137\n",
      "step: 969 0.9109911918640137\n",
      "step: 970 0.9109911918640137\n",
      "step: 971 0.9109911918640137\n",
      "step: 972 0.9109911918640137\n",
      "step: 973 0.9109911918640137\n",
      "step: 974 0.9109911918640137\n",
      "step: 975 0.9109911918640137\n",
      "step: 976 0.9109911918640137\n",
      "step: 977 0.9109911918640137\n",
      "step: 978 0.9109911918640137\n",
      "step: 979 0.9109911918640137\n",
      "step: 980 0.9109911918640137\n",
      "step: 981 0.9109911918640137\n",
      "step: 982 0.9109911918640137\n",
      "step: 983 0.9109911918640137\n",
      "step: 984 0.9109911918640137\n",
      "step: 985 0.9109911918640137\n",
      "step: 986 0.9109911918640137\n",
      "step: 987 0.9109911918640137\n",
      "step: 988 0.9109911918640137\n",
      "step: 989 0.9109911918640137\n",
      "step: 990 0.9109911918640137\n",
      "step: 991 0.9109911918640137\n",
      "step: 992 0.9109911918640137\n",
      "step: 993 0.9109911918640137\n",
      "step: 994 0.9109911918640137\n",
      "step: 995 0.9109911918640137\n",
      "step: 996 0.9109911918640137\n",
      "step: 997 0.9109911918640137\n",
      "step: 998 0.9109911918640137\n",
      "step: 999 0.9109911918640137\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "weights = tf.Variable([[0.3],\n",
    "                       [0.3],\n",
    "                       [0.3],\n",
    "                       #[0.3],\n",
    "                       [0.3]\n",
    "                      ])\n",
    "steps = 1000\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "opt = tf.optimizers.SGD(lr)\n",
    "\n",
    "#Speed up the train step by precompiling\n",
    "@tf.function()\n",
    "def train_step(opt):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y  = tf.matmul(X_train, weights)[:,0]\n",
    "        loss = tf.reduce_mean((tf.math.log1p(y) - y_train) ** 2)\n",
    "    grads = tape.gradient(loss, weights)\n",
    "    opt.apply_gradients([(grads, weights)])\n",
    "    \n",
    "    return loss\n",
    "\n",
    "prev_loss = 9999\n",
    "for i in range(steps):\n",
    "    loss = train_step(opt)\n",
    "    if loss > prev_loss:\n",
    "        lr /= 2\n",
    "        opt.lr = lr\n",
    "        \n",
    "    prev_loss = loss\n",
    "    print(f'step: {i} {loss.numpy()}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE\n",
    "## 2019-12-16 : 0.95496225 blend 1234\n",
    "## 2019-12-18 : 0.9547418 blend 1234\n",
    "## 2019-12-19 : 0.9513415 blend 12345\n",
    "## 2019-12-19 : 0.95134723 blend 1345\n",
    "## 2019-12-19 : 0.95151687 blend 2345\n",
    "## 2019-12-19 : 0.9515808 blend 345\n",
    "## 2019-12-19 : 0.9544586 blend 1236\n",
    "np.sqrt(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights are: w1=0.091667734, w2=0.040769067, w3=0.38903582, w4=0.3867224\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_feather(os.path.join(root, 'sample_submission.feather'))\n",
    "\n",
    "ws = weights.numpy()\n",
    "\n",
    "w1 = ws[0,0]\n",
    "w2 = ws[1,0]\n",
    "w3 = ws[2,0]\n",
    "w4 = ws[3,0]\n",
    "#w5 = ws[4,0]\n",
    "print(\"The weights are: w1=\" + str(w1) + \", w2=\" + str(w2) + \", w3=\" + str(w3) + \", w4=\" + str(w4))\n",
    "\n",
    "sample_submission['meter_reading'] = w1 * test_df.pred1 +  w2 * test_df.pred2  + w3 * test_df.pred3 + w4 * test_df.pred6\n",
    "sample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission_sgd_blend_4_final.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df = leak_df[['meter_reading', 'row_id']].set_index('row_id').dropna()\n",
    "sample_submission.loc[leak_df.index, 'meter_reading'] = leak_df['meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission_sgd_blend_4_final_replaced.csv', index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
